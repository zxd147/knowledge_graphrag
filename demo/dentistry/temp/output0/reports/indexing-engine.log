02:29:19,864 graphrag.index.cli INFO Logging enabled at output/reports/indexing-engine.log
02:29:19,865 graphrag.index.cli INFO Starting pipeline run for: 20240907-022919, dryrun=False
02:29:19,866 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "Qwen2-7B-Instruct",
        "max_tokens": 6000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 3000.0,
        "api_base": "http://192.168.0.245:5008/v1",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 2.0,
        "num_threads": 2
    },
    "async_mode": "threaded",
    "root_dir": ".",
    "reporting": {
        "type": "file",
        "base_dir": "output/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "output/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "${GRAPHRAG_EMBEDDING_MODEL}",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://192.168.0.245:6008/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 2.0,
            "num_threads": 2
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 600,
        "overlap": 150,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "Qwen2-7B-Instruct",
            "max_tokens": 6000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 3000.0,
            "api_base": "http://192.168.0.245:5008/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 2.0,
            "num_threads": 2
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "\u5e74\u9f84\u7fa4\u4f53",
            "\u6cbb\u7597\u65b9\u6cd5",
            "\u75c7\u72b6",
            "\u5065\u5eb7\u72b6\u51b5",
            "\u75be\u75c5\u540d\u79f0",
            "\u68c0\u67e5\u9879\u76ee",
            "\u836f\u7269\u540d\u79f0",
            "\u751f\u6d3b\u65b9\u5f0f",
            "\u98ce\u9669\u56e0\u7d20",
            "\u9884\u9632\u63aa\u65bd",
            "\u533b\u7597\u673a\u6784",
            "\u4e13\u79d1\u9886\u57df",
            "\u75c5\u7406\u7c7b\u578b",
            "\u4e34\u5e8a\u8bd5\u9a8c",
            "\u8425\u517b\u7d20",
            "\u5fc3\u7406\u72b6\u6001",
            "\u5eb7\u590d\u63aa\u65bd",
            "\u6ce8\u610f\u4e8b\u9879",
            "\u6cbb\u7597\u65b9\u6848"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "Qwen2-7B-Instruct",
            "max_tokens": 6000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 3000.0,
            "api_base": "http://192.168.0.245:5008/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 2.0,
            "num_threads": 2
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "Qwen2-7B-Instruct",
            "max_tokens": 6000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 3000.0,
            "api_base": "http://192.168.0.245:5008/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 2.0,
            "num_threads": 2
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 5000,
        "max_input_length": 3000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "Qwen2-7B-Instruct",
            "max_tokens": 6000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 3000.0,
            "api_base": "http://192.168.0.245:5008/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 2.0,
            "num_threads": 2
        },
        "async_mode": "threaded",
        "enabled": true,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
02:29:19,866 graphrag.index.create_pipeline_config INFO skipping workflows 
02:29:19,866 graphrag.index.run INFO Running pipeline
02:29:19,866 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at output/artifacts
02:29:19,866 graphrag.index.input.load_input INFO loading input from root_dir=input
02:29:19,866 graphrag.index.input.load_input INFO using file storage for input
02:29:19,867 graphrag.index.storage.file_pipeline_storage INFO search input for files matching .*\.txt$
02:29:19,867 graphrag.index.input.text INFO found text files from input, found [('dentistry.txt', {})]
02:29:19,868 graphrag.index.input.text INFO Found 1 files, loading 1
02:29:19,868 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_final_covariates', 'create_summarized_entities', 'join_text_units_to_covariate_ids', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
02:29:19,869 graphrag.index.run INFO Final # of rows loaded: 1
02:29:19,944 graphrag.index.run INFO Running workflow: create_base_text_units...
02:29:19,945 graphrag.index.run INFO dependencies for create_base_text_units: []
02:29:19,947 datashaper.workflow.workflow INFO executing verb orderby
02:29:19,948 datashaper.workflow.workflow INFO executing verb zip
02:29:19,950 datashaper.workflow.workflow INFO executing verb aggregate_override
02:29:19,953 datashaper.workflow.workflow INFO executing verb chunk
02:29:20,41 datashaper.workflow.workflow INFO executing verb select
02:29:20,43 datashaper.workflow.workflow INFO executing verb unroll
02:29:20,46 datashaper.workflow.workflow INFO executing verb rename
02:29:20,48 datashaper.workflow.workflow INFO executing verb genid
02:29:20,50 datashaper.workflow.workflow INFO executing verb unzip
02:29:20,53 datashaper.workflow.workflow INFO executing verb copy
02:29:20,56 datashaper.workflow.workflow INFO executing verb filter
02:29:20,58 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
02:29:20,146 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
02:29:20,146 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
02:29:20,146 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
02:29:20,154 datashaper.workflow.workflow INFO executing verb entity_extract
02:29:20,155 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://192.168.0.245:5008/v1
02:29:20,166 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for Qwen2-7B-Instruct: TPM=0, RPM=0
02:29:20,166 graphrag.index.llm.load_llm INFO create concurrency limiter for Qwen2-7B-Instruct: 25
02:29:49,746 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:29:49,749 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.57609151900033. input_tokens=3894, output_tokens=2109
02:30:14,303 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:30:14,304 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 54.135589994999464. input_tokens=3894, output_tokens=4022
02:30:20,83 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:30:20,84 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.33316859900151. input_tokens=55, output_tokens=2214
02:30:44,616 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:30:44,617 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.311572511000122. input_tokens=55, output_tokens=2377
02:30:53,244 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:30:53,245 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.15505995199783. input_tokens=3893, output_tokens=2388
02:31:22,365 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:31:22,366 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 29.120323336999718. input_tokens=55, output_tokens=2179
02:31:59,545 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:31:59,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.17369300399878. input_tokens=3894, output_tokens=2964
02:32:31,531 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:32:31,532 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 31.985307393999392. input_tokens=55, output_tokens=2328
02:33:11,804 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:33:11,806 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 147.18182634299956. input_tokens=3894, output_tokens=10453
02:33:31,606 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:33:31,607 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.80012756600263. input_tokens=55, output_tokens=1290
02:33:48,677 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:33:48,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 17.064884076000453. input_tokens=3894, output_tokens=1290
02:34:19,621 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:34:19,622 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.94334326199896. input_tokens=55, output_tokens=2224
02:34:43,622 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:34:43,623 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.995121883999673. input_tokens=3894, output_tokens=1771
02:34:57,752 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:34:57,753 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 14.128803116997005. input_tokens=55, output_tokens=1055
02:35:01,144 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:35:01,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 149.60629794800116. input_tokens=3894, output_tokens=10941
02:35:31,594 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:35:31,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.83574503199998. input_tokens=3894, output_tokens=2467
02:36:03,974 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:36:03,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 32.37922652099951. input_tokens=55, output_tokens=2392
02:36:24,788 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:36:24,789 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.807544848998077. input_tokens=3722, output_tokens=1382
02:36:43,105 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:36:43,107 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.316749138997693. input_tokens=55, output_tokens=1193
02:37:34,635 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:37:34,637 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 153.48895150500175. input_tokens=55, output_tokens=10856
02:37:34,645 datashaper.workflow.workflow INFO executing verb merge_graphs
02:37:34,658 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
02:37:34,748 graphrag.index.run INFO Running workflow: create_final_covariates...
02:37:34,749 graphrag.index.run INFO dependencies for create_final_covariates: ['create_base_text_units']
02:37:34,749 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
02:37:34,757 datashaper.workflow.workflow INFO executing verb extract_covariates
02:37:46,429 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:37:46,429 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.668263803003356. input_tokens=2114, output_tokens=543
02:37:58,1 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:37:58,2 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.572031196999887. input_tokens=32, output_tokens=543
02:38:01,632 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:38:01,633 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.87412884099831. input_tokens=2114, output_tokens=1774
02:38:04,577 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:38:04,578 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.57049944100072. input_tokens=2113, output_tokens=282
02:38:28,330 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:38:28,331 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 26.697010322001006. input_tokens=32, output_tokens=1774
02:38:28,675 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:38:28,676 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.3397195110010216. input_tokens=2114, output_tokens=5
02:38:29,17 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:38:29,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 0.3408266670012381. input_tokens=32, output_tokens=5
02:38:49,589 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:38:49,590 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.566752736001945. input_tokens=2114, output_tokens=1202
02:39:02,79 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:39:02,80 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.488754607998999. input_tokens=32, output_tokens=748
02:39:12,653 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:39:12,654 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.569116892998863. input_tokens=2114, output_tokens=706
02:39:25,346 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:39:25,347 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.691333443002804. input_tokens=32, output_tokens=807
02:39:48,867 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:39:48,871 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.515451177001523. input_tokens=2116, output_tokens=1203
02:40:12,693 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:40:12,693 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 23.82173694100129. input_tokens=32, output_tokens=1203
02:40:31,324 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:40:31,326 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 146.74652107499787. input_tokens=32, output_tokens=9037
02:40:37,554 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:40:37,554 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.223319942000671. input_tokens=2115, output_tokens=282
02:40:41,918 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:40:41,919 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.21995328900084. input_tokens=2114, output_tokens=1720
02:41:00,481 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:00,482 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.926351479000004. input_tokens=32, output_tokens=1352
02:41:10,764 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:10,765 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 28.845326407998073. input_tokens=32, output_tokens=1718
02:41:16,915 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:16,916 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 16.429106187999423. input_tokens=1943, output_tokens=924
02:41:33,276 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:33,276 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 16.35955097600163. input_tokens=32, output_tokens=924
02:41:33,284 datashaper.workflow.workflow INFO executing verb window
02:41:33,287 datashaper.workflow.workflow INFO executing verb genid
02:41:33,291 datashaper.workflow.workflow INFO executing verb convert
02:41:33,294 datashaper.workflow.workflow INFO executing verb rename
02:41:33,298 datashaper.workflow.workflow INFO executing verb select
02:41:33,299 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_covariates.parquet
02:41:33,410 graphrag.index.run INFO Running workflow: create_summarized_entities...
02:41:33,410 graphrag.index.run INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
02:41:33,410 graphrag.index.run INFO read table from storage: create_base_extracted_entities.parquet
02:41:33,422 datashaper.workflow.workflow INFO executing verb summarize_descriptions
02:41:35,932 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:35,933 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.504284386999643. input_tokens=608, output_tokens=230
02:41:36,172 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:36,173 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7419912980003573. input_tokens=539, output_tokens=226
02:41:37,950 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:37,950 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0121563139982754. input_tokens=449, output_tokens=175
02:41:38,568 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:38,569 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6128166690032231. input_tokens=400, output_tokens=45
02:41:39,837 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:39,838 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.659669237000344. input_tokens=583, output_tokens=346
02:41:40,834 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:40,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2611307719998877. input_tokens=387, output_tokens=159
02:41:42,687 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:42,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8476412609998079. input_tokens=583, output_tokens=176
02:41:44,254 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:44,255 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5618585089978296. input_tokens=423, output_tokens=146
02:41:44,518 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:44,519 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.675274124998396. input_tokens=568, output_tokens=426
02:41:45,748 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:45,749 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4854430810009944. input_tokens=431, output_tokens=134
02:41:45,848 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:45,849 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3248238439991837. input_tokens=422, output_tokens=85
02:41:48,105 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:48,106 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.351621460999013. input_tokens=569, output_tokens=232
02:41:49,604 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:49,605 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7500188539997907. input_tokens=619, output_tokens=308
02:41:50,629 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:50,629 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5182530140009476. input_tokens=509, output_tokens=253
02:41:54,524 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:54,525 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.9150769150001. input_tokens=613, output_tokens=458
02:41:57,853 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:57,854 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.218895930000144. input_tokens=503, output_tokens=637
02:41:58,309 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:41:58,310 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.778876925000077. input_tokens=527, output_tokens=281
02:42:02,317 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:02,318 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.458487231000618. input_tokens=518, output_tokens=428
02:42:02,341 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:02,342 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.026948793001793. input_tokens=516, output_tokens=308
02:42:04,314 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:04,315 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9718357300007483. input_tokens=516, output_tokens=195
02:42:06,810 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:06,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.490925927999342. input_tokens=532, output_tokens=221
02:42:07,78 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:07,78 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.754508780002652. input_tokens=554, output_tokens=379
02:42:09,234 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:09,234 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1507143620001443. input_tokens=515, output_tokens=187
02:42:10,69 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:10,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.252316686000995. input_tokens=539, output_tokens=290
02:42:10,381 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:10,382 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1421217490023992. input_tokens=519, output_tokens=110
02:42:12,168 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:12,169 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.0939176280007814. input_tokens=488, output_tokens=186
02:42:13,836 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:13,837 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.449823863000347. input_tokens=517, output_tokens=317
02:42:16,98 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:16,99 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.9247259049989225. input_tokens=608, output_tokens=373
02:42:16,959 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:16,959 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.116960041999846. input_tokens=604, output_tokens=303
02:42:17,839 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:17,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7356039579972276. input_tokens=503, output_tokens=159
02:42:18,883 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:18,883 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9185531670009368. input_tokens=481, output_tokens=135
02:42:19,834 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:19,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9897254429997702. input_tokens=452, output_tokens=149
02:42:21,263 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:21,264 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.423335229002987. input_tokens=468, output_tokens=116
02:42:23,117 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:23,117 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.228722021998692. input_tokens=800, output_tokens=362
02:42:23,140 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:23,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.872326291999343. input_tokens=549, output_tokens=154
02:42:24,851 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:24,852 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7095328409996. input_tokens=465, output_tokens=147
02:42:28,405 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:28,406 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5488485640016734. input_tokens=572, output_tokens=289
02:42:29,347 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:29,348 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.225420506998489. input_tokens=490, output_tokens=541
02:42:30,277 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:30,278 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8663642299979983. input_tokens=493, output_tokens=148
02:42:31,693 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:31,693 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4098171639998327. input_tokens=412, output_tokens=142
02:42:32,878 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:32,879 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5255760730033217. input_tokens=437, output_tokens=284
02:42:34,71 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:34,72 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.373207913002261. input_tokens=423, output_tokens=180
02:42:34,851 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:34,851 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.967032685999584. input_tokens=383, output_tokens=170
02:42:35,995 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:35,995 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9182144490005157. input_tokens=394, output_tokens=145
02:42:37,246 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:37,247 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.390677768999012. input_tokens=383, output_tokens=185
02:42:38,403 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:38,403 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1508752039990213. input_tokens=432, output_tokens=103
02:42:39,184 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:39,185 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.182121283000015. input_tokens=421, output_tokens=254
02:42:42,87 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:42,88 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6793131020021974. input_tokens=421, output_tokens=309
02:42:42,727 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:42,728 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.537643104999006. input_tokens=406, output_tokens=341
02:42:45,464 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:45,465 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.371607226003107. input_tokens=417, output_tokens=328
02:42:45,565 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:45,565 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8310076620000473. input_tokens=453, output_tokens=229
02:42:47,242 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:47,243 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7702145110015408. input_tokens=498, output_tokens=137
02:42:48,329 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:48,330 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7592461240019475. input_tokens=466, output_tokens=233
02:42:48,756 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:48,757 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.508540096001525. input_tokens=426, output_tokens=122
02:42:52,417 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:52,417 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.082182636997459. input_tokens=426, output_tokens=367
02:42:53,176 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:53,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7531583249983669. input_tokens=425, output_tokens=72
02:42:54,953 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:54,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7713907149991428. input_tokens=416, output_tokens=132
02:42:55,573 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:55,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.811192643999675. input_tokens=429, output_tokens=594
02:42:56,661 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:56,662 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7031706679990748. input_tokens=479, output_tokens=139
02:42:58,409 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:58,410 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.830523592001555. input_tokens=442, output_tokens=196
02:42:59,145 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:42:59,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4762728070018056. input_tokens=428, output_tokens=227
02:43:00,44 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:43:00,45 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6280141170027491. input_tokens=463, output_tokens=149
02:43:01,131 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:43:01,132 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.98107999900094. input_tokens=410, output_tokens=173
02:43:03,615 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:43:03,616 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5653935549999005. input_tokens=414, output_tokens=250
02:43:04,45 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:43:04,46 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9081901559984544. input_tokens=418, output_tokens=264
02:43:06,482 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:43:06,483 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.861529908997909. input_tokens=417, output_tokens=229
02:43:06,983 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:43:06,984 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9322621900028025. input_tokens=414, output_tokens=251
02:43:09,160 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:43:09,161 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.67212094199931. input_tokens=417, output_tokens=222
02:43:09,305 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:43:09,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.316793803998735. input_tokens=414, output_tokens=187
02:43:11,457 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:43:11,458 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.291685714000778. input_tokens=411, output_tokens=191
02:43:11,715 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:43:11,716 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4052435849989706. input_tokens=415, output_tokens=223
02:43:11,729 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
02:43:11,824 graphrag.index.run INFO Running workflow: join_text_units_to_covariate_ids...
02:43:11,825 graphrag.index.run INFO dependencies for join_text_units_to_covariate_ids: ['create_final_covariates']
02:43:11,825 graphrag.index.run INFO read table from storage: create_final_covariates.parquet
02:43:11,835 datashaper.workflow.workflow INFO executing verb select
02:43:11,840 datashaper.workflow.workflow INFO executing verb aggregate_override
02:43:11,841 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_covariate_ids.parquet
02:43:11,937 graphrag.index.run INFO Running workflow: create_base_entity_graph...
02:43:11,937 graphrag.index.run INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
02:43:11,938 graphrag.index.run INFO read table from storage: create_summarized_entities.parquet
02:43:11,948 datashaper.workflow.workflow INFO executing verb cluster_graph
02:43:11,997 datashaper.workflow.workflow INFO executing verb select
02:43:11,999 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
02:43:12,95 graphrag.index.run INFO Running workflow: create_final_entities...
02:43:12,96 graphrag.index.run INFO dependencies for create_final_entities: ['create_base_entity_graph']
02:43:12,96 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
02:43:12,109 datashaper.workflow.workflow INFO executing verb unpack_graph
02:43:12,130 datashaper.workflow.workflow INFO executing verb rename
02:43:12,136 datashaper.workflow.workflow INFO executing verb select
02:43:12,141 datashaper.workflow.workflow INFO executing verb dedupe
02:43:12,147 datashaper.workflow.workflow INFO executing verb rename
02:43:12,153 datashaper.workflow.workflow INFO executing verb filter
02:43:12,160 datashaper.workflow.workflow INFO executing verb text_split
02:43:12,168 datashaper.workflow.workflow INFO executing verb drop
02:43:12,174 datashaper.workflow.workflow INFO executing verb merge
02:43:12,200 datashaper.workflow.workflow INFO executing verb text_embed
02:43:12,201 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://192.168.0.245:6008/v1
02:43:12,212 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for ${GRAPHRAG_EMBEDDING_MODEL}: TPM=0, RPM=0
02:43:12,212 graphrag.index.llm.load_llm INFO create concurrency limiter for ${GRAPHRAG_EMBEDDING_MODEL}: 25
02:43:12,220 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 224 inputs via 224 snippets using 14 batches. max_batch_size=16, max_tokens=8191
02:43:12,403 httpx INFO HTTP Request: POST http://192.168.0.245:6008/v1/embeddings "HTTP/1.1 200 OK"
02:43:12,607 httpx INFO HTTP Request: POST http://192.168.0.245:6008/v1/embeddings "HTTP/1.1 200 OK"
02:43:12,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.44627879899780964. input_tokens=701, output_tokens=0
02:43:12,746 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.524464280999382. input_tokens=1631, output_tokens=0
02:43:12,979 httpx INFO HTTP Request: POST http://192.168.0.245:6008/v1/embeddings "HTTP/1.1 200 OK"
02:43:13,39 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3557123700011289. input_tokens=3934, output_tokens=0
02:43:13,258 httpx INFO HTTP Request: POST http://192.168.0.245:6008/v1/embeddings "HTTP/1.1 200 OK"
02:43:13,501 httpx INFO HTTP Request: POST http://192.168.0.245:6008/v1/embeddings "HTTP/1.1 200 OK"
02:43:13,566 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8104905279978993. input_tokens=3472, output_tokens=0
02:43:13,633 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5799913189985091. input_tokens=2768, output_tokens=0
02:43:13,839 httpx INFO HTTP Request: POST http://192.168.0.245:6008/v1/embeddings "HTTP/1.1 200 OK"
02:43:13,899 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.25074283399953856. input_tokens=1211, output_tokens=0
02:43:14,39 httpx INFO HTTP Request: POST http://192.168.0.245:6008/v1/embeddings "HTTP/1.1 200 OK"
02:43:14,205 httpx INFO HTTP Request: POST http://192.168.0.245:6008/v1/embeddings "HTTP/1.1 200 OK"
02:43:14,264 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6138469200013787. input_tokens=1540, output_tokens=0
02:43:14,328 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4147578779993637. input_tokens=583, output_tokens=0
02:43:14,517 httpx INFO HTTP Request: POST http://192.168.0.245:6008/v1/embeddings "HTTP/1.1 200 OK"
02:43:14,578 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.23548559100163402. input_tokens=656, output_tokens=0
02:43:14,695 httpx INFO HTTP Request: POST http://192.168.0.245:6008/v1/embeddings "HTTP/1.1 200 OK"
02:43:14,896 httpx INFO HTTP Request: POST http://192.168.0.245:6008/v1/embeddings "HTTP/1.1 200 OK"
02:43:14,956 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6119182879992877. input_tokens=745, output_tokens=0
02:43:15,20 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4264059359993553. input_tokens=1425, output_tokens=0
02:43:15,233 httpx INFO HTTP Request: POST http://192.168.0.245:6008/v1/embeddings "HTTP/1.1 200 OK"
02:43:15,295 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.2611469349976687. input_tokens=1192, output_tokens=0
02:43:15,421 httpx INFO HTTP Request: POST http://192.168.0.245:6008/v1/embeddings "HTTP/1.1 200 OK"
02:43:15,587 httpx INFO HTTP Request: POST http://192.168.0.245:6008/v1/embeddings "HTTP/1.1 200 OK"
02:43:15,644 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.608440708998387. input_tokens=1098, output_tokens=0
02:43:15,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3986404429997492. input_tokens=412, output_tokens=0
02:43:15,729 datashaper.workflow.workflow INFO executing verb drop
02:43:15,736 datashaper.workflow.workflow INFO executing verb filter
02:43:15,745 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
02:43:15,872 graphrag.index.run INFO Running workflow: create_final_nodes...
02:43:15,872 graphrag.index.run INFO dependencies for create_final_nodes: ['create_base_entity_graph']
02:43:15,873 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
02:43:15,889 datashaper.workflow.workflow INFO executing verb layout_graph
02:43:15,955 datashaper.workflow.workflow INFO executing verb unpack_graph
02:43:15,981 datashaper.workflow.workflow INFO executing verb unpack_graph
02:43:16,6 datashaper.workflow.workflow INFO executing verb drop
02:43:16,14 datashaper.workflow.workflow INFO executing verb filter
02:43:16,26 datashaper.workflow.workflow INFO executing verb select
02:43:16,33 datashaper.workflow.workflow INFO executing verb rename
02:43:16,41 datashaper.workflow.workflow INFO executing verb convert
02:43:16,50 datashaper.workflow.workflow INFO executing verb join
02:43:16,60 datashaper.workflow.workflow INFO executing verb rename
02:43:16,61 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
02:43:16,168 graphrag.index.run INFO Running workflow: create_final_communities...
02:43:16,168 graphrag.index.run INFO dependencies for create_final_communities: ['create_base_entity_graph']
02:43:16,168 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
02:43:16,188 datashaper.workflow.workflow INFO executing verb unpack_graph
02:43:16,212 datashaper.workflow.workflow INFO executing verb unpack_graph
02:43:16,237 datashaper.workflow.workflow INFO executing verb aggregate_override
02:43:16,247 datashaper.workflow.workflow INFO executing verb join
02:43:16,260 datashaper.workflow.workflow INFO executing verb join
02:43:16,271 datashaper.workflow.workflow INFO executing verb concat
02:43:16,280 datashaper.workflow.workflow INFO executing verb filter
02:43:16,313 datashaper.workflow.workflow INFO executing verb aggregate_override
02:43:16,325 datashaper.workflow.workflow INFO executing verb join
02:43:16,337 datashaper.workflow.workflow INFO executing verb filter
02:43:16,348 datashaper.workflow.workflow INFO executing verb fill
02:43:16,358 datashaper.workflow.workflow INFO executing verb merge
02:43:16,370 datashaper.workflow.workflow INFO executing verb copy
02:43:16,380 datashaper.workflow.workflow INFO executing verb select
02:43:16,381 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
02:43:16,493 graphrag.index.run INFO Running workflow: join_text_units_to_entity_ids...
02:43:16,493 graphrag.index.run INFO dependencies for join_text_units_to_entity_ids: ['create_final_entities']
02:43:16,493 graphrag.index.run INFO read table from storage: create_final_entities.parquet
02:43:16,520 datashaper.workflow.workflow INFO executing verb select
02:43:16,530 datashaper.workflow.workflow INFO executing verb unroll
02:43:16,541 datashaper.workflow.workflow INFO executing verb aggregate_override
02:43:16,544 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_entity_ids.parquet
02:43:16,654 graphrag.index.run INFO Running workflow: create_final_relationships...
02:43:16,655 graphrag.index.run INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
02:43:16,655 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
02:43:16,657 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
02:43:16,679 datashaper.workflow.workflow INFO executing verb unpack_graph
02:43:16,706 datashaper.workflow.workflow INFO executing verb filter
02:43:16,721 datashaper.workflow.workflow INFO executing verb rename
02:43:16,732 datashaper.workflow.workflow INFO executing verb filter
02:43:16,747 datashaper.workflow.workflow INFO executing verb drop
02:43:16,770 datashaper.workflow.workflow INFO executing verb compute_edge_combined_degree
02:43:16,784 datashaper.workflow.workflow INFO executing verb convert
02:43:16,795 datashaper.workflow.workflow INFO executing verb convert
02:43:16,796 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
02:43:16,909 graphrag.index.run INFO Running workflow: join_text_units_to_relationship_ids...
02:43:16,910 graphrag.index.run INFO dependencies for join_text_units_to_relationship_ids: ['create_final_relationships']
02:43:16,910 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
02:43:16,934 datashaper.workflow.workflow INFO executing verb select
02:43:16,946 datashaper.workflow.workflow INFO executing verb unroll
02:43:16,959 datashaper.workflow.workflow INFO executing verb aggregate_override
02:43:16,972 datashaper.workflow.workflow INFO executing verb select
02:43:16,973 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_relationship_ids.parquet
02:43:17,84 graphrag.index.run INFO Running workflow: create_final_community_reports...
02:43:17,85 graphrag.index.run INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships', 'create_final_covariates']
02:43:17,85 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
02:43:17,87 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
02:43:17,89 graphrag.index.run INFO read table from storage: create_final_covariates.parquet
02:43:17,114 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
02:43:17,131 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
02:43:17,145 datashaper.workflow.workflow INFO executing verb prepare_community_reports_claims
02:43:17,159 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
02:43:17,174 datashaper.workflow.workflow INFO executing verb prepare_community_reports
02:43:17,175 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 224
02:43:17,186 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 224
02:43:17,216 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 224
02:43:17,251 datashaper.workflow.workflow INFO executing verb create_community_reports
02:47:28,584 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:47:28,584 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:47:28,584 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
02:47:29,177 graphrag.llm.openai.utils INFO success load json in step 3
02:47:29,177 graphrag.llm.openai.utils INFO success load json in step 4
02:47:29,178 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 251.919449447003. input_tokens=3798, output_tokens=3792
02:47:29,179 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
02:47:29,766 graphrag.llm.openai.utils INFO success load json in step 3
02:47:29,766 graphrag.llm.openai.utils INFO success load json in step 4
02:47:29,768 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 252.51244700099778. input_tokens=5679, output_tokens=5509
02:54:15,10 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:54:15,10 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:54:15,10 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
02:54:15,595 graphrag.llm.openai.utils INFO success load json in step 3
02:54:15,596 graphrag.llm.openai.utils INFO success load json in step 4
02:54:15,596 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 405.668793437002. input_tokens=4334, output_tokens=2990
02:54:15,597 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
02:54:16,183 graphrag.llm.openai.utils INFO success load json in step 3
02:54:16,183 graphrag.llm.openai.utils INFO success load json in step 4
02:54:16,185 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 406.2591504230004. input_tokens=5198, output_tokens=10407
02:59:42,455 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:59:42,456 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
02:59:42,456 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
02:59:43,69 graphrag.llm.openai.utils INFO success load json in step 3
02:59:43,69 graphrag.llm.openai.utils INFO success load json in step 4
02:59:43,69 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 326.8653189659999. input_tokens=3613, output_tokens=2554
02:59:43,70 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
02:59:43,659 graphrag.llm.openai.utils INFO success load json in step 3
02:59:43,659 graphrag.llm.openai.utils INFO success load json in step 4
02:59:43,660 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 327.4586103519978. input_tokens=3926, output_tokens=3391
03:08:24,868 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:08:24,869 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:08:24,869 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
03:08:25,462 graphrag.llm.openai.utils INFO success load json in step 3
03:08:25,462 graphrag.llm.openai.utils INFO success load json in step 4
03:08:25,464 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 521.7827176520004. input_tokens=3750, output_tokens=8918
03:08:25,464 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
03:08:26,71 graphrag.llm.openai.utils INFO success load json in step 3
03:08:26,71 graphrag.llm.openai.utils INFO success load json in step 4
03:08:26,73 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 522.3956629050008. input_tokens=4022, output_tokens=4017
03:13:09,232 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:13:09,232 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:13:09,232 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
03:13:09,840 graphrag.llm.openai.utils INFO success load json in step 3
03:13:09,840 graphrag.llm.openai.utils INFO success load json in step 4
03:13:09,841 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 283.74949811600163. input_tokens=4002, output_tokens=909
03:13:09,841 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
03:13:10,431 graphrag.llm.openai.utils INFO success load json in step 3
03:13:10,431 graphrag.llm.openai.utils INFO success load json in step 4
03:13:10,431 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 284.34334058999957. input_tokens=4096, output_tokens=719
03:19:37,332 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:19:37,333 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:19:37,333 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
03:19:37,949 graphrag.llm.openai.utils INFO success load json in step 3
03:19:37,949 graphrag.llm.openai.utils INFO success load json in step 4
03:19:37,950 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 387.49842087399884. input_tokens=5739, output_tokens=4821
03:19:37,951 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
03:19:38,544 graphrag.llm.openai.utils INFO success load json in step 3
03:19:38,544 graphrag.llm.openai.utils INFO success load json in step 4
03:19:38,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 388.0966055459976. input_tokens=4479, output_tokens=13181
03:20:26,857 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:20:26,858 graphrag.llm.openai.utils INFO success load json in step 1
03:20:26,859 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 48.296416344001045. input_tokens=4004, output_tokens=1026
03:20:44,645 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:20:44,646 graphrag.llm.openai.utils INFO success load json in step 1
03:20:44,646 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 66.08056392299841. input_tokens=4653, output_tokens=1452
03:25:41,471 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:25:41,472 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
03:25:42,66 graphrag.llm.openai.utils INFO success load json in step 3
03:25:42,66 graphrag.llm.openai.utils INFO success load json in step 4
03:25:42,67 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 315.19222593099767. input_tokens=6156, output_tokens=3306
03:25:56,929 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:25:56,930 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
03:25:57,507 graphrag.llm.openai.utils INFO success load json in step 3
03:25:57,508 graphrag.llm.openai.utils INFO success load json in step 4
03:25:57,508 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 312.84633948899864. input_tokens=7434, output_tokens=1011
03:29:01,238 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:29:01,239 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
03:29:01,839 graphrag.llm.openai.utils INFO success load json in step 3
03:29:01,839 graphrag.llm.openai.utils INFO success load json in step 4
03:29:01,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 199.75648360500054. input_tokens=7483, output_tokens=3870
03:34:13,754 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:34:13,754 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:34:13,755 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
03:34:14,361 graphrag.llm.openai.utils INFO success load json in step 3
03:34:14,361 graphrag.llm.openai.utils INFO success load json in step 4
03:34:14,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 312.46981873799814. input_tokens=4773, output_tokens=761
03:34:14,362 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
03:34:14,944 graphrag.llm.openai.utils INFO success load json in step 3
03:34:14,944 graphrag.llm.openai.utils INFO success load json in step 4
03:34:14,944 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 313.05545411099956. input_tokens=4171, output_tokens=762
03:39:36,985 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:39:36,985 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:39:36,986 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
03:39:37,557 graphrag.llm.openai.utils INFO success load json in step 3
03:39:37,557 graphrag.llm.openai.utils INFO success load json in step 4
03:39:37,557 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 322.5947406029991. input_tokens=4790, output_tokens=785
03:39:37,558 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
03:39:38,151 graphrag.llm.openai.utils INFO success load json in step 3
03:39:38,151 graphrag.llm.openai.utils INFO success load json in step 4
03:39:38,151 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 323.1918186369985. input_tokens=4454, output_tokens=1034
03:48:48,692 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:48:48,692 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:48:48,693 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
03:48:49,287 graphrag.llm.openai.utils INFO success load json in step 3
03:48:49,287 graphrag.llm.openai.utils INFO success load json in step 4
03:48:49,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 551.1147107079996. input_tokens=9772, output_tokens=1009
03:48:49,288 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
03:48:49,883 graphrag.llm.openai.utils INFO success load json in step 3
03:57:26,518 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:57:26,518 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
03:57:26,519 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
03:57:27,93 graphrag.llm.openai.utils INFO success load json in step 3
03:57:27,93 graphrag.llm.openai.utils INFO success load json in step 4
03:57:27,93 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 516.6253477159989. input_tokens=5546, output_tokens=1223
03:57:27,94 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
03:57:27,660 graphrag.llm.openai.utils INFO success load json in step 3
04:04:26,949 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
04:04:26,949 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
04:04:27,515 graphrag.llm.openai.utils INFO success load json in step 3
04:11:41,958 httpx INFO HTTP Request: POST http://192.168.0.245:5008/v1/chat/completions "HTTP/1.1 200 OK"
04:11:41,959 graphrag.llm.openai.utils WARNING Warning: Error decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
04:11:42,509 graphrag.llm.openai.utils INFO success load json in step 3
04:11:43,71 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "/home/zxd/code/Chat/graphrag/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "/home/zxd/code/Chat/graphrag/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/home/zxd/code/Chat/graphrag/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/home/zxd/code/Chat/graphrag/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/home/zxd/code/Chat/graphrag/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/home/zxd/code/Chat/graphrag/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/home/zxd/code/Chat/graphrag/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/home/zxd/.conda/envs/grag/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/home/zxd/.conda/envs/grag/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/home/zxd/.conda/envs/grag/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/home/zxd/.conda/envs/grag/lib/python3.10/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
  File "/home/zxd/.conda/envs/grag/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/home/zxd/.conda/envs/grag/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/home/zxd/code/Chat/graphrag/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/home/zxd/code/Chat/graphrag/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/home/zxd/code/Chat/graphrag/graphrag/llm/base/base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
  File "/home/zxd/code/Chat/graphrag/graphrag/llm/openai/openai_chat_llm.py", line 90, in _invoke_json
    raise RuntimeError(error_msg)
RuntimeError: Failed to generate valid JSON output - Faulty JSON: {}
04:11:43,72 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
04:11:43,72 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 0
04:11:43,96 datashaper.workflow.workflow INFO executing verb window
04:11:43,98 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
04:11:43,227 graphrag.index.run INFO Running workflow: create_final_text_units...
04:11:43,227 graphrag.index.run INFO dependencies for create_final_text_units: ['join_text_units_to_relationship_ids', 'join_text_units_to_covariate_ids', 'create_base_text_units', 'join_text_units_to_entity_ids']
04:11:43,227 graphrag.index.run INFO read table from storage: join_text_units_to_relationship_ids.parquet
04:11:43,229 graphrag.index.run INFO read table from storage: join_text_units_to_covariate_ids.parquet
04:11:43,230 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
04:11:43,232 graphrag.index.run INFO read table from storage: join_text_units_to_entity_ids.parquet
04:11:43,258 datashaper.workflow.workflow INFO executing verb select
04:11:43,271 datashaper.workflow.workflow INFO executing verb rename
04:11:43,284 datashaper.workflow.workflow INFO executing verb join
04:11:43,299 datashaper.workflow.workflow INFO executing verb join
04:11:43,315 datashaper.workflow.workflow INFO executing verb join
04:11:43,331 datashaper.workflow.workflow INFO executing verb aggregate_override
04:11:43,345 datashaper.workflow.workflow INFO executing verb select
04:11:43,346 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
04:11:43,455 graphrag.index.run INFO Running workflow: create_base_documents...
04:11:43,456 graphrag.index.run INFO dependencies for create_base_documents: ['create_final_text_units']
04:11:43,456 graphrag.index.run INFO read table from storage: create_final_text_units.parquet
04:11:43,485 datashaper.workflow.workflow INFO executing verb unroll
04:11:43,500 datashaper.workflow.workflow INFO executing verb select
04:11:43,514 datashaper.workflow.workflow INFO executing verb rename
04:11:43,527 datashaper.workflow.workflow INFO executing verb join
04:11:43,544 datashaper.workflow.workflow INFO executing verb aggregate_override
04:11:43,559 datashaper.workflow.workflow INFO executing verb join
04:11:43,575 datashaper.workflow.workflow INFO executing verb rename
04:11:43,590 datashaper.workflow.workflow INFO executing verb convert
04:11:43,591 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
04:11:43,704 graphrag.index.run INFO Running workflow: create_final_documents...
04:11:43,719 graphrag.index.run INFO dependencies for create_final_documents: ['create_base_documents']
04:11:43,720 graphrag.index.run INFO read table from storage: create_base_documents.parquet
04:11:43,750 datashaper.workflow.workflow INFO executing verb rename
04:11:43,751 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
04:11:43,799 graphrag.index.cli INFO All workflows completed successfully.
