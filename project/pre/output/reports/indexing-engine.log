16:02:44,256 graphrag.index.cli INFO Logging enabled at output/reports/indexing-engine.log
16:02:44,257 graphrag.index.cli INFO Starting pipeline run for: 20241223-160244, dryrun=False
16:02:44,257 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "Qwen/Qwen2.5-72B-Instruct",
        "max_tokens": 8192,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 3000.0,
        "api_base": "http://192.168.0.244:8016/v1",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 1.0,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": ".",
    "reporting": {
        "type": "file",
        "base_dir": "output/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "output/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "m3e-large",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://192.168.0.244:8016/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 1.0,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 500,
        "overlap": 200,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "Qwen/Qwen2.5-72B-Instruct",
            "max_tokens": 8192,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 3000.0,
            "api_base": "http://192.168.0.244:8016/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 1.0,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "Qwen/Qwen2.5-72B-Instruct",
            "max_tokens": 8192,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 3000.0,
            "api_base": "http://192.168.0.244:8016/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 1.0,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "Qwen/Qwen2.5-72B-Instruct",
            "max_tokens": 8192,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 3000.0,
            "api_base": "http://192.168.0.244:8016/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 3.0,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 8192,
        "max_input_length": 6000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "Qwen/Qwen2.5-72B-Instruct",
            "max_tokens": 8192,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 3000.0,
            "api_base": "http://192.168.0.244:8016/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 1.0,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": true,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
16:02:44,258 graphrag.index.create_pipeline_config INFO skipping workflows 
16:02:44,258 graphrag.index.run INFO Running pipeline
16:02:44,258 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at output/artifacts
16:02:44,258 graphrag.index.input.load_input INFO loading input from root_dir=input
16:02:44,258 graphrag.index.input.load_input INFO using file storage for input
16:02:44,259 graphrag.index.storage.file_pipeline_storage INFO search input for files matching .*\.txt$
16:02:44,259 graphrag.index.input.text INFO found text files from input, found [('Spectral-detection-knowledge.txt', {})]
16:02:44,260 graphrag.index.input.text INFO Found 1 files, loading 1
16:02:44,260 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_final_covariates', 'create_summarized_entities', 'join_text_units_to_covariate_ids', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
16:02:44,261 graphrag.index.run INFO Final # of rows loaded: 1
16:02:44,358 graphrag.index.run INFO Running workflow: create_base_text_units...
16:02:44,358 graphrag.index.run INFO dependencies for create_base_text_units: []
16:02:44,360 datashaper.workflow.workflow INFO executing verb orderby
16:02:44,362 datashaper.workflow.workflow INFO executing verb zip
16:02:44,364 datashaper.workflow.workflow INFO executing verb aggregate_override
16:02:44,366 datashaper.workflow.workflow INFO executing verb chunk
16:02:44,455 datashaper.workflow.workflow INFO executing verb select
16:02:44,457 datashaper.workflow.workflow INFO executing verb unroll
16:02:44,474 datashaper.workflow.workflow INFO executing verb rename
16:02:44,476 datashaper.workflow.workflow INFO executing verb genid
16:02:44,479 datashaper.workflow.workflow INFO executing verb unzip
16:02:44,481 datashaper.workflow.workflow INFO executing verb copy
16:02:44,484 datashaper.workflow.workflow INFO executing verb filter
16:02:44,486 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
16:02:44,635 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
16:02:44,635 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
16:02:44,635 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
16:02:44,654 datashaper.workflow.workflow INFO executing verb entity_extract
16:02:44,661 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://192.168.0.244:8016/v1
16:02:44,673 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for Qwen/Qwen2.5-72B-Instruct: TPM=0, RPM=0
16:02:44,673 graphrag.index.llm.load_llm INFO create concurrency limiter for Qwen/Qwen2.5-72B-Instruct: 25
16:02:56,807 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:02:56,808 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.123340796679258. input_tokens=1948, output_tokens=1014
16:03:57,839 graphrag.index.cli INFO Logging enabled at output/reports/indexing-engine.log
16:03:57,841 graphrag.index.cli INFO Starting pipeline run for: 20241223-160357, dryrun=False
16:03:57,841 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "Qwen/Qwen2.5-72B-Instruct",
        "max_tokens": 8192,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 3000.0,
        "api_base": "http://192.168.0.244:8016/v1",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 1.0,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": ".",
    "reporting": {
        "type": "file",
        "base_dir": "output/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "output/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "m3e-large",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://192.168.0.244:8016/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 1.0,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 500,
        "overlap": 200,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "Qwen/Qwen2.5-72B-Instruct",
            "max_tokens": 8192,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 3000.0,
            "api_base": "http://192.168.0.244:8016/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 1.0,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "Qwen/Qwen2.5-72B-Instruct",
            "max_tokens": 8192,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 3000.0,
            "api_base": "http://192.168.0.244:8016/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 1.0,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "Qwen/Qwen2.5-72B-Instruct",
            "max_tokens": 8192,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 3000.0,
            "api_base": "http://192.168.0.244:8016/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 3.0,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 8192,
        "max_input_length": 6000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "Qwen/Qwen2.5-72B-Instruct",
            "max_tokens": 8192,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 3000.0,
            "api_base": "http://192.168.0.244:8016/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 1.0,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": true,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
16:03:57,842 graphrag.index.create_pipeline_config INFO skipping workflows 
16:03:57,842 graphrag.index.run INFO Running pipeline
16:03:57,842 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at output/artifacts
16:03:57,842 graphrag.index.input.load_input INFO loading input from root_dir=input
16:03:57,842 graphrag.index.input.load_input INFO using file storage for input
16:03:57,842 graphrag.index.storage.file_pipeline_storage INFO search input for files matching .*\.txt$
16:03:57,842 graphrag.index.input.text INFO found text files from input, found [('Spectral-detection-knowledge.txt', {})]
16:03:57,843 graphrag.index.input.text INFO Found 1 files, loading 1
16:03:57,844 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_final_covariates', 'create_summarized_entities', 'join_text_units_to_covariate_ids', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
16:03:57,845 graphrag.index.run INFO Final # of rows loaded: 1
16:03:57,941 graphrag.index.run INFO Running workflow: create_base_text_units...
16:03:57,942 graphrag.index.run INFO Skipping create_base_text_units because it already exists
16:03:58,37 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
16:03:58,37 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
16:03:58,37 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
16:03:58,44 datashaper.workflow.workflow INFO executing verb entity_extract
16:03:58,45 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://192.168.0.244:8016/v1
16:03:58,56 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for Qwen/Qwen2.5-72B-Instruct: TPM=0, RPM=0
16:03:58,56 graphrag.index.llm.load_llm INFO create concurrency limiter for Qwen/Qwen2.5-72B-Instruct: 25
16:04:15,258 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:04:15,259 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 17.118015320971608. input_tokens=1947, output_tokens=1076
16:04:21,507 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:04:21,507 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.35973776411265. input_tokens=1949, output_tokens=1092
16:04:24,16 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:04:24,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.756729371845722. input_tokens=55, output_tokens=511
16:04:25,938 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:04:25,940 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.7897812705487. input_tokens=1948, output_tokens=1319
16:04:38,743 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:04:38,744 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.803930949419737. input_tokens=55, output_tokens=994
16:04:42,833 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:04:42,834 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.325903504155576. input_tokens=55, output_tokens=857
16:04:42,876 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:04:42,877 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 18.84141199849546. input_tokens=1948, output_tokens=1542
16:04:44,36 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:04:44,37 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 45.881720210425556. input_tokens=55, output_tokens=1337
16:04:46,354 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:04:46,355 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 48.20928867626935. input_tokens=1947, output_tokens=1827
16:04:48,25 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:04:48,26 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.277766112238169. input_tokens=1947, output_tokens=828
16:04:51,528 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:04:51,529 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.65181439369917. input_tokens=55, output_tokens=365
16:04:54,355 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:04:54,356 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.314903927966952. input_tokens=1946, output_tokens=861
16:05:02,984 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:05:02,985 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.627465235069394. input_tokens=55, output_tokens=636
16:05:03,31 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:05:03,32 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.19427705090493. input_tokens=1948, output_tokens=1494
16:05:07,703 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:05:07,703 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.676683857105672. input_tokens=55, output_tokens=823
16:05:10,358 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:05:10,358 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 24.002413533627987. input_tokens=55, output_tokens=959
16:05:15,563 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:05:15,564 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.530741336755455. input_tokens=55, output_tokens=859
16:05:18,189 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:05:18,190 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.483232039958239. input_tokens=1947, output_tokens=838
16:05:21,467 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:05:21,468 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 18.479482212103903. input_tokens=1949, output_tokens=974
16:05:31,36 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:05:31,37 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.50382109824568. input_tokens=1946, output_tokens=1108
16:05:32,178 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:05:32,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 10.710068914107978. input_tokens=55, output_tokens=754
16:05:41,451 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:05:41,452 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 23.260538764297962. input_tokens=55, output_tokens=919
16:05:46,200 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:05:46,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 14.016889155842364. input_tokens=1948, output_tokens=1178
16:05:47,418 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:05:47,419 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.850461250171065. input_tokens=1947, output_tokens=1490
16:05:49,459 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:05:49,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.097616660408676. input_tokens=1948, output_tokens=1265
16:06:05,193 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:06:05,194 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.156030169688165. input_tokens=55, output_tokens=1450
16:06:07,33 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:06:07,34 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.83175732754171. input_tokens=55, output_tokens=1061
16:06:08,657 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:06:08,658 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.19683843664825. input_tokens=55, output_tokens=1186
16:06:10,911 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:06:10,912 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.45597578212619. input_tokens=1947, output_tokens=1318
16:06:33,151 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:06:33,151 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.489201003685594. input_tokens=1947, output_tokens=1241
16:06:33,927 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:06:33,928 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.88983895536512. input_tokens=1947, output_tokens=1083
16:06:34,576 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:06:34,577 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.378936828114092. input_tokens=1947, output_tokens=1270
16:06:41,283 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:06:41,284 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.37103571370244. input_tokens=55, output_tokens=1249
16:06:43,72 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:06:43,74 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 9.14491671230644. input_tokens=55, output_tokens=430
16:06:43,324 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:06:43,325 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 10.172358450479805. input_tokens=55, output_tokens=470
16:06:46,195 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:06:46,196 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.1188109600916505. input_tokens=1467, output_tokens=128
16:06:50,696 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:06:50,697 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.499589577317238. input_tokens=55, output_tokens=315
16:06:55,126 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:06:55,127 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.549374839290977. input_tokens=55, output_tokens=761
16:07:02,168 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:07:02,169 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 74.74885982088745. input_tokens=55, output_tokens=1118
16:07:02,703 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:07:02,704 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.416065669618547. input_tokens=1765, output_tokens=850
16:07:11,335 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:07:11,336 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.63109415024519. input_tokens=55, output_tokens=761
16:07:11,342 datashaper.workflow.workflow INFO executing verb merge_graphs
16:07:11,358 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
16:07:11,456 graphrag.index.run INFO Running workflow: create_final_covariates...
16:07:11,457 graphrag.index.run INFO dependencies for create_final_covariates: ['create_base_text_units']
16:07:11,457 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
16:07:11,462 datashaper.workflow.workflow INFO executing verb extract_covariates
16:07:22,657 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:07:22,657 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.157418041490018. input_tokens=1960, output_tokens=650
16:07:27,299 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:07:27,300 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.809028517454863. input_tokens=1959, output_tokens=637
16:07:29,176 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:07:29,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 17.678826852701604. input_tokens=1960, output_tokens=903
16:07:35,173 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:07:35,174 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.67809913866222. input_tokens=1961, output_tokens=1090
16:07:36,269 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:07:36,269 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.968961869366467. input_tokens=32, output_tokens=322
16:07:39,650 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:07:39,650 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.3771679466590285. input_tokens=1960, output_tokens=211
16:07:41,20 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:07:41,21 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.843131207861006. input_tokens=32, output_tokens=613
16:07:42,357 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:07:42,358 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.864517962560058. input_tokens=1959, output_tokens=1003
16:07:50,152 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:07:50,153 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 27.494980215094984. input_tokens=32, output_tokens=1248
16:07:55,306 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:07:55,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.149653413332999. input_tokens=1960, output_tokens=340
16:07:59,670 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:07:59,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.01924234442413. input_tokens=32, output_tokens=884
16:08:00,962 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:08:00,964 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 25.788670460693538. input_tokens=32, output_tokens=1830
16:08:03,358 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:08:03,358 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.684472450055182. input_tokens=1959, output_tokens=136
16:08:08,443 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:08:08,443 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.41868862323463. input_tokens=1959, output_tokens=1250
16:08:09,844 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:08:09,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 27.48592183738947. input_tokens=32, output_tokens=1674
16:08:14,306 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:08:14,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.8628271128982306. input_tokens=32, output_tokens=343
16:08:17,206 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:08:17,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 16.239631566219032. input_tokens=1959, output_tokens=904
16:08:20,643 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:08:20,644 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 25.33692982979119. input_tokens=32, output_tokens=1171
16:08:22,266 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:08:22,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.417332107201219. input_tokens=1961, output_tokens=909
16:08:28,149 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:08:28,150 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 24.790142567828298. input_tokens=32, output_tokens=782
16:08:33,493 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:08:33,494 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.846267505548894. input_tokens=1960, output_tokens=1135
16:08:41,143 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:08:41,144 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 23.93588957004249. input_tokens=32, output_tokens=933
16:08:41,281 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:08:41,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.96931521873921. input_tokens=1959, output_tokens=1140
16:08:44,326 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:08:44,327 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.059052480384707. input_tokens=32, output_tokens=928
16:08:51,942 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:08:51,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.612517390400171. input_tokens=1959, output_tokens=278
16:08:54,633 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:08:54,634 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.4805070431903. input_tokens=1959, output_tokens=1108
16:09:01,726 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:09:01,727 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 28.231061999686062. input_tokens=32, output_tokens=1234
16:09:05,57 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:09:05,58 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.90993308927864. input_tokens=1960, output_tokens=839
16:09:12,14 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:09:12,15 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.73145068064332. input_tokens=32, output_tokens=1072
16:09:15,586 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:09:15,587 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.856708446517587. input_tokens=1959, output_tokens=1109
16:09:16,37 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:09:16,38 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.40271707251668. input_tokens=32, output_tokens=1742
16:09:22,31 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:09:22,31 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.989898132160306. input_tokens=1959, output_tokens=226
16:09:26,218 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:09:26,218 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 14.199934357777238. input_tokens=1959, output_tokens=782
16:09:30,378 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:09:30,379 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 14.790727764368057. input_tokens=32, output_tokens=1106
16:09:33,371 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:09:33,372 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.427943017333746. input_tokens=32, output_tokens=3114
16:09:33,751 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:09:33,752 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.532390819862485. input_tokens=32, output_tokens=282
16:09:38,775 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:09:38,776 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.392880693078041. input_tokens=1777, output_tokens=588
16:09:51,831 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:09:51,832 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 18.456319510005414. input_tokens=1479, output_tokens=540
16:09:53,599 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:09:53,601 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 48.54179994855076. input_tokens=32, output_tokens=1383
16:09:54,447 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:09:54,447 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.614130200818181. input_tokens=32, output_tokens=173
16:09:55,802 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:09:55,803 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.77074493933469. input_tokens=32, output_tokens=1425
16:10:02,325 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:02,326 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 23.549880786798894. input_tokens=32, output_tokens=709
16:10:02,331 datashaper.workflow.workflow INFO executing verb window
16:10:02,333 datashaper.workflow.workflow INFO executing verb genid
16:10:02,335 datashaper.workflow.workflow INFO executing verb convert
16:10:02,337 datashaper.workflow.workflow INFO executing verb rename
16:10:02,340 datashaper.workflow.workflow INFO executing verb select
16:10:02,341 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_covariates.parquet
16:10:02,446 graphrag.index.run INFO Running workflow: create_summarized_entities...
16:10:02,446 graphrag.index.run INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
16:10:02,446 graphrag.index.run INFO read table from storage: create_base_extracted_entities.parquet
16:10:02,453 datashaper.workflow.workflow INFO executing verb summarize_descriptions
16:10:03,952 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:03,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4749884437769651. input_tokens=400, output_tokens=47
16:10:04,402 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:04,402 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9222950907424092. input_tokens=535, output_tokens=141
16:10:04,910 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:04,911 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4270591028034687. input_tokens=440, output_tokens=135
16:10:04,972 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:04,972 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4906397657468915. input_tokens=437, output_tokens=98
16:10:06,446 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:06,447 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5317344889044762. input_tokens=459, output_tokens=116
16:10:06,730 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:06,730 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7733833538368344. input_tokens=422, output_tokens=116
16:10:06,800 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:06,800 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.393927736207843. input_tokens=417, output_tokens=177
16:10:07,192 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:07,192 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2188349487259984. input_tokens=428, output_tokens=78
16:10:08,330 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:08,330 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5957123301923275. input_tokens=424, output_tokens=110
16:10:08,744 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:08,745 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.293939797207713. input_tokens=443, output_tokens=100
16:10:10,182 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:10,183 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.706859258003533. input_tokens=1017, output_tokens=423
16:10:10,264 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:10,265 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5155216893181205. input_tokens=445, output_tokens=114
16:10:11,221 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:11,221 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8871145928278565. input_tokens=474, output_tokens=127
16:10:11,249 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:11,250 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.448012108914554. input_tokens=568, output_tokens=209
16:10:12,173 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:12,173 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.976760931313038. input_tokens=468, output_tokens=224
16:10:12,506 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:12,506 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3191598262637854. input_tokens=449, output_tokens=120
16:10:12,800 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:12,800 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.534550487063825. input_tokens=397, output_tokens=96
16:10:12,882 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:12,883 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.657247070223093. input_tokens=438, output_tokens=116
16:10:14,182 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:14,183 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6723956763744354. input_tokens=467, output_tokens=132
16:10:14,632 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:14,632 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.381254017353058. input_tokens=475, output_tokens=140
16:10:14,862 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:14,862 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6832307251170278. input_tokens=474, output_tokens=129
16:10:15,312 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:15,312 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4282349590212107. input_tokens=448, output_tokens=137
16:10:15,933 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:15,934 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0670931674540043. input_tokens=393, output_tokens=31
16:10:16,172 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:16,172 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.3675409350544214. input_tokens=431, output_tokens=150
16:10:16,545 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:16,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.356536230072379. input_tokens=472, output_tokens=146
16:10:17,294 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:17,295 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6585606429725885. input_tokens=488, output_tokens=121
16:10:18,329 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:18,330 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.779765447601676. input_tokens=436, output_tokens=130
16:10:18,546 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:18,547 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.2295083105564117. input_tokens=425, output_tokens=125
16:10:18,964 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:18,965 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7887719944119453. input_tokens=429, output_tokens=97
16:10:19,441 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:19,442 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.141757547855377. input_tokens=414, output_tokens=99
16:10:19,713 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:19,714 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.379487100057304. input_tokens=457, output_tokens=83
16:10:20,608 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:20,609 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.671487935818732. input_tokens=465, output_tokens=214
16:10:21,576 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:21,577 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1308626951649785. input_tokens=517, output_tokens=152
16:10:21,863 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:21,864 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.312452861107886. input_tokens=463, output_tokens=143
16:10:22,26 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:22,26 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4132329048588872. input_tokens=450, output_tokens=101
16:10:23,548 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:23,549 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.579439589753747. input_tokens=591, output_tokens=185
16:10:23,828 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:23,829 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9610573230311275. input_tokens=531, output_tokens=160
16:10:24,361 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:24,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7804352128878236. input_tokens=434, output_tokens=84
16:10:24,532 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:24,532 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.812060101889074. input_tokens=718, output_tokens=236
16:10:25,222 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:25,223 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.390344007872045. input_tokens=400, output_tokens=57
16:10:25,645 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:25,646 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6151315374299884. input_tokens=448, output_tokens=149
16:10:25,946 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:25,947 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3935096757486463. input_tokens=471, output_tokens=98
16:10:25,978 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:25,979 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6131203323602676. input_tokens=425, output_tokens=111
16:10:27,107 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:27,107 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4571935841813684. input_tokens=413, output_tokens=50
16:10:27,734 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:27,735 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.198080450296402. input_tokens=451, output_tokens=163
16:10:28,639 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:28,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6884064376354218. input_tokens=443, output_tokens=126
16:10:28,939 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:28,940 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.713104181922972. input_tokens=440, output_tokens=147
16:10:29,353 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:29,354 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.3741244887933135. input_tokens=430, output_tokens=131
16:10:29,369 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:29,370 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.631312225945294. input_tokens=432, output_tokens=78
16:10:29,431 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:29,431 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3196503557264805. input_tokens=454, output_tokens=87
16:10:30,166 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:30,167 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2229714589193463. input_tokens=450, output_tokens=95
16:10:30,207 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:30,208 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5646571163088083. input_tokens=449, output_tokens=100
16:10:30,604 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:30,605 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2468795767053962. input_tokens=439, output_tokens=76
16:10:31,829 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:31,830 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6571636367589235. input_tokens=397, output_tokens=42
16:10:32,248 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:32,249 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8781458539888263. input_tokens=438, output_tokens=131
16:10:32,370 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:32,371 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.938287129625678. input_tokens=446, output_tokens=137
16:10:32,817 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:32,817 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2062138728797436. input_tokens=470, output_tokens=160
16:10:33,467 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:33,467 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.2567991698160768. input_tokens=449, output_tokens=97
16:10:33,830 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:33,830 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0085949273779988. input_tokens=409, output_tokens=33
16:10:33,969 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:33,970 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1362601425498724. input_tokens=448, output_tokens=172
16:10:34,443 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:34,444 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.068568952381611. input_tokens=409, output_tokens=78
16:10:34,720 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:34,721 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.24957465659827. input_tokens=414, output_tokens=58
16:10:35,275 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:35,275 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.02200426440686. input_tokens=444, output_tokens=135
16:10:35,742 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:35,743 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7680385578423738. input_tokens=450, output_tokens=146
16:10:36,563 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:36,564 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.116199783049524. input_tokens=456, output_tokens=73
16:10:36,666 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:36,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.832790789194405. input_tokens=439, output_tokens=101
16:10:37,800 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:37,801 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0763268833979964. input_tokens=435, output_tokens=120
16:10:38,171 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:38,171 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.891900895163417. input_tokens=418, output_tokens=142
16:10:38,329 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:38,329 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.761389099061489. input_tokens=475, output_tokens=120
16:10:38,710 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:38,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.03987536393106. input_tokens=626, output_tokens=170
16:10:40,652 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:40,653 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.906003188341856. input_tokens=481, output_tokens=194
16:10:40,671 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:40,672 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8665374033153057. input_tokens=457, output_tokens=89
16:10:40,887 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:40,887 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1723139388486743. input_tokens=534, output_tokens=156
16:10:42,139 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:42,140 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.466607409529388. input_tokens=481, output_tokens=146
16:10:42,970 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:42,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.637596762739122. input_tokens=476, output_tokens=125
16:10:42,994 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:42,995 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.819430792704225. input_tokens=507, output_tokens=116
16:10:43,335 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:43,336 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6786881890147924. input_tokens=481, output_tokens=129
16:10:44,271 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:44,271 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.379543388262391. input_tokens=474, output_tokens=153
16:10:45,854 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:45,854 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8788847643882036. input_tokens=472, output_tokens=156
16:10:46,523 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:46,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2480298709124327. input_tokens=489, output_tokens=118
16:10:46,535 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:46,536 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1959070581942797. input_tokens=468, output_tokens=120
16:10:46,560 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:46,560 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5641006929799914. input_tokens=498, output_tokens=186
16:10:47,961 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:47,961 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.817070804536343. input_tokens=479, output_tokens=211
16:10:48,160 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:48,161 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6333445636555552. input_tokens=462, output_tokens=118
16:10:49,0 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:49,1 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.439048674888909. input_tokens=463, output_tokens=118
16:10:49,119 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:49,119 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.582850743085146. input_tokens=470, output_tokens=88
16:10:49,921 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:49,921 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.062716938555241. input_tokens=472, output_tokens=117
16:10:50,217 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:50,218 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.052993622608483. input_tokens=485, output_tokens=133
16:10:50,787 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:50,788 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7829494401812553. input_tokens=491, output_tokens=169
16:10:51,476 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:51,477 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.511297171935439. input_tokens=487, output_tokens=170
16:10:51,703 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:51,704 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4824173729866743. input_tokens=454, output_tokens=93
16:10:51,924 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:51,925 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.801616142503917. input_tokens=555, output_tokens=148
16:10:52,616 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:52,616 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8234621081501245. input_tokens=484, output_tokens=131
16:10:53,12 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:53,12 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.530989688821137. input_tokens=489, output_tokens=106
16:10:53,91 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:53,92 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1660001892596483. input_tokens=499, output_tokens=83
16:10:54,387 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:54,387 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2944668363779783. input_tokens=495, output_tokens=105
16:10:54,407 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:54,408 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3915421469137073. input_tokens=515, output_tokens=108
16:10:54,422 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:54,422 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7138044219464064. input_tokens=564, output_tokens=127
16:10:55,877 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:55,878 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.257272547110915. input_tokens=493, output_tokens=131
16:10:55,972 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:55,973 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5495783388614655. input_tokens=470, output_tokens=118
16:10:56,308 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:56,309 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.379240814596415. input_tokens=611, output_tokens=246
16:10:56,799 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:56,799 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3902988424524665. input_tokens=487, output_tokens=118
16:10:57,539 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:57,539 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.657012633047998. input_tokens=474, output_tokens=110
16:10:57,710 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:57,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7374042812734842. input_tokens=489, output_tokens=104
16:10:58,967 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:58,968 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6549405017867684. input_tokens=478, output_tokens=170
16:10:59,12 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:10:59,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.621594148688018. input_tokens=552, output_tokens=217
16:11:00,834 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:11:00,834 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.029455767013133. input_tokens=490, output_tokens=129
16:11:00,846 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
16:11:00,954 graphrag.index.run INFO Running workflow: join_text_units_to_covariate_ids...
16:11:00,954 graphrag.index.run INFO dependencies for join_text_units_to_covariate_ids: ['create_final_covariates']
16:11:00,954 graphrag.index.run INFO read table from storage: create_final_covariates.parquet
16:11:00,962 datashaper.workflow.workflow INFO executing verb select
16:11:00,965 datashaper.workflow.workflow INFO executing verb aggregate_override
16:11:00,967 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_covariate_ids.parquet
16:11:01,75 graphrag.index.run INFO Running workflow: create_base_entity_graph...
16:11:01,76 graphrag.index.run INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
16:11:01,76 graphrag.index.run INFO read table from storage: create_summarized_entities.parquet
16:11:01,84 datashaper.workflow.workflow INFO executing verb cluster_graph
16:11:01,659 datashaper.workflow.workflow INFO executing verb select
16:11:01,660 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
16:11:01,775 graphrag.index.run INFO Running workflow: create_final_entities...
16:11:01,775 graphrag.index.run INFO dependencies for create_final_entities: ['create_base_entity_graph']
16:11:01,775 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
16:11:01,786 datashaper.workflow.workflow INFO executing verb unpack_graph
16:11:01,809 datashaper.workflow.workflow INFO executing verb rename
16:11:01,813 datashaper.workflow.workflow INFO executing verb select
16:11:01,817 datashaper.workflow.workflow INFO executing verb dedupe
16:11:01,821 datashaper.workflow.workflow INFO executing verb rename
16:11:01,825 datashaper.workflow.workflow INFO executing verb filter
16:11:01,830 datashaper.workflow.workflow INFO executing verb text_split
16:11:01,837 datashaper.workflow.workflow INFO executing verb drop
16:11:01,842 datashaper.workflow.workflow INFO executing verb merge
16:11:01,864 datashaper.workflow.workflow INFO executing verb text_embed
16:11:02,53 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://192.168.0.244:8016/v1
16:11:02,64 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for m3e-large: TPM=0, RPM=0
16:11:02,64 graphrag.index.llm.load_llm INFO create concurrency limiter for m3e-large: 25
16:11:02,71 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 225 inputs via 225 snippets using 15 batches. max_batch_size=16, max_tokens=8191
16:11:03,113 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/embeddings "HTTP/1.1 200 OK"
16:11:03,113 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/embeddings "HTTP/1.1 200 OK"
16:11:03,117 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/embeddings "HTTP/1.1 200 OK"
16:11:03,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1020747506991029. input_tokens=1415, output_tokens=0
16:11:03,184 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/embeddings "HTTP/1.1 200 OK"
16:11:03,184 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/embeddings "HTTP/1.1 200 OK"
16:11:03,243 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1681226389482617. input_tokens=1002, output_tokens=0
16:11:03,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2306023249402642. input_tokens=1230, output_tokens=0
16:11:03,376 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3035943917930126. input_tokens=1611, output_tokens=0
16:11:03,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3656708598136902. input_tokens=1321, output_tokens=0
16:11:03,651 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/embeddings "HTTP/1.1 200 OK"
16:11:03,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.2581657264381647. input_tokens=956, output_tokens=0
16:11:03,850 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/embeddings "HTTP/1.1 200 OK"
16:11:03,910 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4564202567562461. input_tokens=1236, output_tokens=0
16:11:04,781 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/embeddings "HTTP/1.1 200 OK"
16:11:04,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1220344081521034. input_tokens=940, output_tokens=0
16:11:04,852 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/embeddings "HTTP/1.1 200 OK"
16:11:04,853 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/embeddings "HTTP/1.1 200 OK"
16:11:04,853 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/embeddings "HTTP/1.1 200 OK"
16:11:04,853 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/embeddings "HTTP/1.1 200 OK"
16:11:04,915 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4569294396787882. input_tokens=737, output_tokens=0
16:11:04,976 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5186402816325426. input_tokens=1015, output_tokens=0
16:11:05,38 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5811242973431945. input_tokens=915, output_tokens=0
16:11:05,99 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1765840845182538. input_tokens=1088, output_tokens=0
16:11:05,299 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/embeddings "HTTP/1.1 200 OK"
16:11:05,358 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.2511922987177968. input_tokens=675, output_tokens=0
16:11:05,487 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/embeddings "HTTP/1.1 200 OK"
16:11:05,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.42927586287260056. input_tokens=933, output_tokens=0
16:11:05,554 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/embeddings "HTTP/1.1 200 OK"
16:11:05,557 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4392850073054433. input_tokens=6, output_tokens=0
16:11:05,568 datashaper.workflow.workflow INFO executing verb drop
16:11:05,573 datashaper.workflow.workflow INFO executing verb filter
16:11:05,576 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
16:11:05,715 graphrag.index.run INFO Running workflow: create_final_nodes...
16:11:05,715 graphrag.index.run INFO dependencies for create_final_nodes: ['create_base_entity_graph']
16:11:05,715 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
16:11:05,729 datashaper.workflow.workflow INFO executing verb layout_graph
16:11:05,819 datashaper.workflow.workflow INFO executing verb unpack_graph
16:11:05,845 datashaper.workflow.workflow INFO executing verb unpack_graph
16:11:05,872 datashaper.workflow.workflow INFO executing verb drop
16:11:05,877 datashaper.workflow.workflow INFO executing verb filter
16:11:05,887 datashaper.workflow.workflow INFO executing verb select
16:11:05,893 datashaper.workflow.workflow INFO executing verb rename
16:11:05,899 datashaper.workflow.workflow INFO executing verb convert
16:11:05,906 datashaper.workflow.workflow INFO executing verb join
16:11:05,916 datashaper.workflow.workflow INFO executing verb rename
16:11:05,917 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
16:11:06,31 graphrag.index.run INFO Running workflow: create_final_communities...
16:11:06,31 graphrag.index.run INFO dependencies for create_final_communities: ['create_base_entity_graph']
16:11:06,31 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
16:11:06,47 datashaper.workflow.workflow INFO executing verb unpack_graph
16:11:06,73 datashaper.workflow.workflow INFO executing verb unpack_graph
16:11:06,99 datashaper.workflow.workflow INFO executing verb aggregate_override
16:11:06,107 datashaper.workflow.workflow INFO executing verb join
16:11:06,118 datashaper.workflow.workflow INFO executing verb join
16:11:06,129 datashaper.workflow.workflow INFO executing verb concat
16:11:06,136 datashaper.workflow.workflow INFO executing verb filter
16:11:06,189 datashaper.workflow.workflow INFO executing verb aggregate_override
16:11:06,199 datashaper.workflow.workflow INFO executing verb join
16:11:06,208 datashaper.workflow.workflow INFO executing verb filter
16:11:06,218 datashaper.workflow.workflow INFO executing verb fill
16:11:06,226 datashaper.workflow.workflow INFO executing verb merge
16:11:06,237 datashaper.workflow.workflow INFO executing verb copy
16:11:06,245 datashaper.workflow.workflow INFO executing verb select
16:11:06,247 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
16:11:06,371 graphrag.index.run INFO Running workflow: join_text_units_to_entity_ids...
16:11:06,371 graphrag.index.run INFO dependencies for join_text_units_to_entity_ids: ['create_final_entities']
16:11:06,371 graphrag.index.run INFO read table from storage: create_final_entities.parquet
16:11:06,397 datashaper.workflow.workflow INFO executing verb select
16:11:06,406 datashaper.workflow.workflow INFO executing verb unroll
16:11:06,415 datashaper.workflow.workflow INFO executing verb aggregate_override
16:11:06,417 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_entity_ids.parquet
16:11:06,536 graphrag.index.run INFO Running workflow: create_final_relationships...
16:11:06,536 graphrag.index.run INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
16:11:06,536 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
16:11:06,540 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
16:11:06,561 datashaper.workflow.workflow INFO executing verb unpack_graph
16:11:06,592 datashaper.workflow.workflow INFO executing verb filter
16:11:06,605 datashaper.workflow.workflow INFO executing verb rename
16:11:06,615 datashaper.workflow.workflow INFO executing verb filter
16:11:06,630 datashaper.workflow.workflow INFO executing verb drop
16:11:06,639 datashaper.workflow.workflow INFO executing verb compute_edge_combined_degree
16:11:06,651 datashaper.workflow.workflow INFO executing verb convert
16:11:06,661 datashaper.workflow.workflow INFO executing verb convert
16:11:06,662 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
16:11:06,786 graphrag.index.run INFO Running workflow: join_text_units_to_relationship_ids...
16:11:06,786 graphrag.index.run INFO dependencies for join_text_units_to_relationship_ids: ['create_final_relationships']
16:11:06,786 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
16:11:06,808 datashaper.workflow.workflow INFO executing verb select
16:11:06,831 datashaper.workflow.workflow INFO executing verb unroll
16:11:06,842 datashaper.workflow.workflow INFO executing verb aggregate_override
16:11:06,854 datashaper.workflow.workflow INFO executing verb select
16:11:06,855 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_relationship_ids.parquet
16:11:06,974 graphrag.index.run INFO Running workflow: create_final_community_reports...
16:11:06,975 graphrag.index.run INFO dependencies for create_final_community_reports: ['create_final_covariates', 'create_final_relationships', 'create_final_nodes']
16:11:06,975 graphrag.index.run INFO read table from storage: create_final_covariates.parquet
16:11:06,978 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
16:11:06,979 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
16:11:07,2 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
16:11:07,16 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
16:11:07,30 datashaper.workflow.workflow INFO executing verb prepare_community_reports_claims
16:11:07,42 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
16:11:07,56 datashaper.workflow.workflow INFO executing verb prepare_community_reports
16:11:07,56 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 225
16:11:07,84 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 225
16:11:07,128 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 225
16:11:07,190 datashaper.workflow.workflow INFO executing verb create_community_reports
16:11:25,129 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:11:25,129 graphrag.llm.openai.utils WARNING Warning: fail to decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
16:11:25,129 graphrag.llm.openai.utils INFO success load json in step 2{'title': '', 'summary': '', 'rating': 8.5, 'rating_explanation': '', 'findings': [{'summary': '', 'explanation': '[records:  (177),  (190, 243)]'}, {'summary': '', 'explanation': '[records:  (177),  (190)]'}, {'summary': '', 'explanation': '[records:  (177),  (190, 243)]'}, {'summary': '', 'explanation': '[records:  (177),  (190)]'}, {'summary': '', 'explanation': '[records:  (177),  (190)]'}, {'summary': '', 'explanation': '[records:  (177),  (190, 243)]'}]}
16:11:25,130 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.70163421239704. input_tokens=5391, output_tokens=1173
16:11:27,905 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:11:27,905 graphrag.llm.openai.utils WARNING Warning: fail to decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
16:11:27,905 graphrag.llm.openai.utils INFO success load json in step 2{'title': '', 'summary': '', 'rating': 8.5, 'rating_explanation': '', 'findings': [{'summary': '', 'explanation': '[records:  (46)]'}, {'summary': '', 'explanation': '[records:  (147)]'}, {'summary': '', 'explanation': '[records:  (80, 83),  (151)]'}, {'summary': '', 'explanation': '[records:  (80, 83)]'}, {'summary': '', 'explanation': '[records:  (80, 83)]'}, {'summary': '', 'explanation': '[records:  (80, 83)]'}, {'summary': '', 'explanation': '[records:  (46, 147)]'}, {'summary': '', 'explanation': '[records:  (80, 83)]'}]}
16:11:27,906 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 20.47084335796535. input_tokens=5188, output_tokens=1464
16:11:39,126 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:11:39,127 graphrag.llm.openai.utils WARNING Warning: fail to decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
16:11:39,127 graphrag.llm.openai.utils INFO success load json in step 2{'title': '', 'summary': '', 'rating': 8.5, 'rating_explanation': '', 'findings': [{'summary': '', 'explanation': '[records:  (97, 105, 107),  (166, 168, 196)]'}, {'summary': '', 'explanation': '[records:  (193, 196, 197, 198, 199),  (191, 192, 193, 194, 195)]'}, {'summary': '', 'explanation': '[records:  (110),  (171, 174, 176, 179, 180)]'}, {'summary': '', 'explanation': '[records:  (121, 122, 123, 125),  (170, 173, 176, 178, 180)]'}, {'summary': '', 'explanation': '[records:  (127, 181),  (181, 182, 183, 184, 185)]'}, {'summary': '', 'explanation': '[records:  (132, 133, 134),  (134, 185, 186, 187, 188)]'}, {'summary': '', 'explanation': '[records:  (105, 107),  (172, 177, 178, 179, 180)]'}, {'summary': '', 'explanation': '[records:  (133, 142, 143),  (183, 186, 187, 190, 191)]'}, {'summary': '', 'explanation': '[records:  (161, 188, 189, 190, 191)]'}]}
16:11:39,128 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 31.688247315585613. input_tokens=11140, output_tokens=2078
16:11:44,131 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:11:44,131 graphrag.llm.openai.utils INFO success load json in step 1{'title': '', 'summary': '', 'rating': 8.5, 'rating_explanation': '', 'findings': [{'summary': '', 'explanation': '[records:  (65, 69, 75),  (119, 143, 125)]'}, {'summary': '', 'explanation': '[records:  (75),  (43, 119, 143)]'}, {'summary': '', 'explanation': '[records:  (69, 67, 68),  (141, 138, 139, 140)]'}, {'summary': '', 'explanation': '[records:  (65),  (125, 127, 131)]'}, {'summary': '', 'explanation': '[records:  (66),  (138, 144, 145)]'}, {'summary': '', 'explanation': '[records:  (76, 77),  (144, 145)]'}]}
16:11:44,132 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.69981884770095. input_tokens=6767, output_tokens=1452
16:11:59,355 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:11:59,356 graphrag.llm.openai.utils INFO success load json in step 1{'title': '', 'summary': '', 'rating': 8.5, 'rating_explanation': '', 'findings': [{'summary': '', 'explanation': '[records:  (175),  (188)]'}, {'summary': '', 'explanation': '[records:  (188, 241)]'}, {'summary': '', 'explanation': '[records:  (183),  (241)]'}, {'summary': '', 'explanation': '[records:  (175),  (188)]'}, {'summary': '', 'explanation': '[records:  (175),  (188)]'}, {'summary': '', 'explanation': '[records:  (175)]'}, {'summary': '', 'explanation': '[records:  (175),  (188)]'}]}
16:11:59,357 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.006058983504772. input_tokens=5270, output_tokens=1123
16:12:02,334 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:12:02,335 graphrag.llm.openai.utils WARNING Warning: fail to decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
16:12:02,335 graphrag.llm.openai.utils INFO success load json in step 2{'title': '', 'summary': '', 'rating': 8.5, 'rating_explanation': '', 'findings': [{'summary': '', 'explanation': '[records:  (176),  (189)]'}, {'summary': '', 'explanation': '[records:  (184),  (197)]'}, {'summary': '', 'explanation': '[records:  (108),  (170)]'}, {'summary': '', 'explanation': '[records:  (109),  (216)]'}, {'summary': '', 'explanation': '[records:  (242)]'}, {'summary': '', 'explanation': '[records:  (176, 184, 108, 109),  (189, 197, 170, 216, 242)]'}]}
16:12:02,335 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.971916298381984. input_tokens=5902, output_tokens=1390
16:12:06,256 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:12:06,256 graphrag.llm.openai.utils WARNING Warning: fail to decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
16:12:06,256 graphrag.llm.openai.utils INFO success load json in step 2{'title': '', 'summary': '', 'rating': 8.5, 'rating_explanation': '', 'findings': [{'summary': '', 'explanation': '[records:  (192),  (198)]'}, {'summary': '', 'explanation': '[records:  (201),  (206)]'}, {'summary': '', 'explanation': '[records:  (244, 245)]'}, {'summary': '', 'explanation': '[records:  (206)]'}, {'summary': '', 'explanation': '[records:  (192)]'}]}
16:12:06,257 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.896536505781114. input_tokens=5319, output_tokens=919
16:12:20,865 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:12:20,866 graphrag.llm.openai.utils WARNING Warning: fail to decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
16:12:20,866 graphrag.llm.openai.utils INFO success load json in step 2{'title': '', 'summary': '', 'rating': 8.3, 'rating_explanation': '', 'findings': [{'summary': '', 'explanation': '[records:  (57)]'}, {'summary': '', 'explanation': '[records:  (34)]'}, {'summary': '', 'explanation': '[records:  (134, 185)]'}, {'summary': '', 'explanation': '[records:  (131),  (135)]'}, {'summary': '', 'explanation': '[records:  (34, 134, 185)]'}, {'summary': '', 'explanation': '[records:  (34, 134, 185)]'}, {'summary': '', 'explanation': '[records: NONE]'}]}
16:12:20,867 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.51256415527314. input_tokens=5445, output_tokens=1437
16:12:21,488 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:12:21,488 graphrag.llm.openai.utils WARNING Warning: fail to decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
16:12:21,488 graphrag.llm.openai.utils INFO success load json in step 2{'title': '', 'summary': '', 'rating': 8.5, 'rating_explanation': '', 'findings': [{'summary': '', 'explanation': '[records:  (98),  (169, 208)]'}, {'summary': '', 'explanation': '[records:  (100),  (167, 209)]'}, {'summary': '', 'explanation': '[records:  (210)]'}, {'summary': '', 'explanation': '[records:  (100),  (167)]'}, {'summary': '', 'explanation': '[records:  (100),  (167, 209)]'}, {'summary': '', 'explanation': '[records:  (98, 100),  (167, 169, 208, 209)]'}]}
16:12:21,489 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 19.13748336303979. input_tokens=5549, output_tokens=1139
16:12:27,402 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:12:27,402 graphrag.llm.openai.utils WARNING Warning: fail to decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
16:12:27,402 graphrag.llm.openai.utils INFO success load json in step 2{'title': '', 'summary': '', 'rating': 9.0, 'rating_explanation': '', 'findings': [{'summary': '', 'explanation': '[records:  (101),  (211, 212, 213, 214)]'}, {'summary': '', 'explanation': '[records:  (172, 210)]'}, {'summary': '', 'explanation': '[records:  (113),  (213)]'}, {'summary': '', 'explanation': '[records:  (111),  (211)]'}, {'summary': '', 'explanation': '[records:  (112),  (212)]'}, {'summary': '', 'explanation': '[records:  (114),  (214)]'}, {'summary': '', 'explanation': '[records:  (210, 172)]'}, {'summary': '', 'explanation': '[records:  (101),  (210, 172)]'}, {'summary': '', 'explanation': '[records:  (101),  (210, 172)]'}]}
16:12:27,403 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 43.04584723711014. input_tokens=5795, output_tokens=1744
16:12:30,938 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:12:30,939 graphrag.llm.openai.utils WARNING Warning: fail to decoding faulty json, attempting repair, attempt to clean up input strings and Markdown frameworks
16:12:30,939 graphrag.llm.openai.utils INFO success load json in step 2{'title': '', 'summary': '', 'rating': 8.5, 'rating_explanation': '', 'findings': [{'summary': '', 'explanation': '[records:  (117),  (175)]'}, {'summary': '', 'explanation': '[records:  (175)]'}, {'summary': '', 'explanation': '[records:  (130),  (217)]'}, {'summary': '', 'explanation': '[records:  (117),  (175)]'}, {'summary': '', 'explanation': '[records:  (117)]'}, {'summary': '', 'explanation': '[records:  (117),  (175)]'}]}
16:12:30,940 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 31.56685182545334. input_tokens=5285, output_tokens=1283
16:12:36,630 httpx INFO HTTP Request: POST http://192.168.0.244:8016/v1/chat/completions "HTTP/1.1 200 OK"
16:12:36,631 graphrag.llm.openai.utils INFO success load json in step 1{'title': '', 'summary': '', 'rating': 9.0, 'rating_explanation': '', 'findings': [{'summary': '', 'explanation': '[records:  (7),  (1)]'}, {'summary': '', 'explanation': '[records:  (12),  (102)]'}, {'summary': '', 'explanation': '[records:  (8),  (100, 105)]'}, {'summary': '', 'explanation': '[records:  (101)]'}, {'summary': '', 'explanation': '[records:  (13),  (103)]'}, {'summary': '', 'explanation': '[records:  (7, 8, 12, 13),  (1, 100, 101, 102, 103, 104, 105)]'}, {'summary': '', 'explanation': '[records:  (7),  (1, 102, 103, 105)]'}]}
16:12:36,631 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 15.748624724335968. input_tokens=5881, output_tokens=1191
